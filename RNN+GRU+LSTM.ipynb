{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3FX9J4MRvS0jVd7ZdPyZl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrashtySoni/DS-ML-DL/blob/main/RNN%2BGRU%2BLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ca65cda94c8"
      },
      "source": [
        "# Recurrent Neural Networks (RNN) with Keras\n",
        "\n",
        "![alt txt](https://docs.google.com/drawings/d/e/2PACX-1vRpQYtOzO1U_3yQLf1885kMaja6MsXtJ8QnlqxrfpTgZmb4WpewJXphGdmotYXDB1VE6zlW6cBY_WqR/pub?w=600&h=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6873211b02d4"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Recurrent neural networks (RNN) are a class of neural networks that is powerful for\n",
        "modeling sequence data such as time series or natural language.\n",
        "\n",
        "Schematically, a RNN layer uses a `for` loop to iterate over the timesteps of a\n",
        "sequence, while maintaining an internal state that encodes information about the\n",
        "timesteps it has seen so far.\n",
        "\n",
        "The Keras RNN API is designed with a focus on:\n",
        "\n",
        "- **Ease of use**: the built-in `keras.layers.RNN`, `keras.layers.LSTM`,\n",
        "`keras.layers.GRU` layers enable you to quickly build recurrent models without\n",
        "having to make difficult configuration choices.\n",
        "\n",
        "- **Ease of customization**: You can also define your own RNN cell layer (the inner\n",
        "part of the `for` loop) with custom behavior, and use it with the generic\n",
        "`keras.layers.RNN` layer (the `for` loop itself). This allows you to quickly\n",
        "prototype different research ideas in a flexible way with minimal code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3600ee25c8e"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71c626bbac35"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98e0c38cf95d"
      },
      "source": [
        "There are three built-in RNN layers in Keras:\n",
        "\n",
        "1. `keras.layers.SimpleRNN`\n",
        "\n",
        "2. `keras.layers.GRU`\n",
        "3. `keras.layers.LSTM`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5617759e54e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aac352c9-beb8-4356-df4a-22890f3beb2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 64)          64000     \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 128)               98816     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 164,106\n",
            "Trainable params: 164,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = keras.Sequential()\n",
        "# Add an Embedding layer expecting input vocab of size 1000, and\n",
        "# output embedding dimension of size 64.\n",
        "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
        "\n",
        "# Add a LSTM layer with 128 internal units.\n",
        "model.add(layers.LSTM(128))\n",
        "\n",
        "# Add a Dense layer with 10 units.\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb8ef33660a0"
      },
      "source": [
        "Built-in RNNs support a number of useful features:\n",
        "\n",
        "- Recurrent dropout, via the `dropout` and `recurrent_dropout` arguments\n",
        "- Ability to process an input sequence in reverse, via the `go_backwards` argument\n",
        "\n",
        "For more information, see the\n",
        "[RNN API documentation](https://keras.io/api/layers/recurrent_layers/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43aa4e4f344d"
      },
      "source": [
        "## Outputs and states\n",
        "\n",
        "By default, the output of a RNN layer contains a single vector per sample. This vector\n",
        "is the RNN cell output corresponding to the last timestep, containing information\n",
        "about the entire input sequence. The shape of this output is `(batch_size, units)`\n",
        "where `units` corresponds to the `units` argument passed to the layer's constructor.\n",
        "\n",
        "A RNN layer can also return the entire sequence of outputs for each sample (one vector\n",
        "per timestep per sample), if you set `return_sequences=True`. The shape of this output\n",
        "is `(batch_size, timesteps, units)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3294dec91e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4db9cbb-3f01-4deb-c358-a796a614f049"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, None, 64)          64000     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, None, 256)         247296    \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 128)               49280     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 361,866\n",
            "Trainable params: 361,866\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = keras.Sequential()\n",
        "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
        "\n",
        "# The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)\n",
        "model.add(layers.GRU(256, return_sequences=True))\n",
        "\n",
        "# The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\n",
        "model.add(layers.SimpleRNN(128))\n",
        "\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU Gated Recurrence Unit RNN with Keras\n",
        "\n",
        "1. What is a Gated Recurrence Unit GRU?\n",
        "<img src=\"https://lh6.googleusercontent.com/GgdN7MLjbqFfHUFFo5iHFU5wvIXUU8hahuCczVL4SCoEXb3TcYXj_HRFU98I9Ir5ghae7yUdkDEHGZYOtaWpKiV7ZSzw-FdRwBhSOa-0FEupTubhKsTrxTvk0lVDkSLq074k4dYH\">\n",
        "2. Creating a Simple GRU RNN with Keras\n",
        "> 1. Importing the Right Modules to Build a GRU in Keras\n",
        "> 2. Adding Layers to Your Gated Recurrence Unit Model\n",
        "3. Training and Testing our GRU RNN on the MNIST Dataset\n",
        "> 1. Load the MNIST dataset\n",
        "> 2. Compile the Gated Recurrence Unit GRU RNN model\n",
        "> 3. Train and Fit the Model\n",
        "> 4. Test your Gated Recurrence Unit RNN Model"
      ],
      "metadata": {
        "id": "-Z1ARh-9awmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2.1 Importing the Right Modules to Build a GRU in Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "g0Sq3DupbftO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.2 Adding Layers to Your GRU RNN Model\n",
        "model = keras.Sequential()\n",
        "model.add(layers.GRU(64, input_shape=(28, 28)))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dense(10))\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_ShM1kab0Ze",
        "outputId": "ef4e4122-c99e-4f02-d610-2350a4a3352c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru (GRU)                   (None, 64)                18048     \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 64)               256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 18,954\n",
            "Trainable params: 18,826\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 Training and Testing our GRU Model on the MNIST Dataset\n",
        "#3.1 Get dataset from keras\n",
        "mnist = keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train/255.0, x_test/255.0\n",
        "x_validate, y_validate = x_test[:-10], y_test[:-10]\n",
        "x_test, y_test = x_test[-10:], y_test[-10:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBGgeTnVb6dq",
        "outputId": "1c2f8929-b729-4047-cacc-734427ef5b20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.2 Compile the Keras GRU RNN\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=\"sgd\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")"
      ],
      "metadata": {
        "id": "t-t-KHo_cC9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.3 Train and Fit the GRU RNN Model\n",
        "model.fit(\n",
        "    x_train, y_train, validation_data=(x_validate, y_validate), batch_size=64, epochs=10\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQQOyih3cJ99",
        "outputId": "e16bd075-bbac-4068-8725-c843a1c8e9dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "938/938 [==============================] - 25s 23ms/step - loss: 1.3912 - accuracy: 0.5389 - val_loss: 1.0638 - val_accuracy: 0.6380\n",
            "Epoch 2/10\n",
            "938/938 [==============================] - 21s 23ms/step - loss: 0.8150 - accuracy: 0.7288 - val_loss: 0.6269 - val_accuracy: 0.7953\n",
            "Epoch 3/10\n",
            "938/938 [==============================] - 19s 20ms/step - loss: 0.5507 - accuracy: 0.8221 - val_loss: 0.7545 - val_accuracy: 0.7464\n",
            "Epoch 4/10\n",
            "938/938 [==============================] - 19s 20ms/step - loss: 0.4152 - accuracy: 0.8705 - val_loss: 0.4555 - val_accuracy: 0.8435\n",
            "Epoch 5/10\n",
            "938/938 [==============================] - 20s 22ms/step - loss: 0.3278 - accuracy: 0.9011 - val_loss: 0.3744 - val_accuracy: 0.8756\n",
            "Epoch 6/10\n",
            "938/938 [==============================] - 21s 23ms/step - loss: 0.2568 - accuracy: 0.9230 - val_loss: 0.2206 - val_accuracy: 0.9338\n",
            "Epoch 7/10\n",
            "938/938 [==============================] - 19s 21ms/step - loss: 0.2113 - accuracy: 0.9368 - val_loss: 0.2276 - val_accuracy: 0.9252\n",
            "Epoch 8/10\n",
            "938/938 [==============================] - 19s 20ms/step - loss: 0.1841 - accuracy: 0.9455 - val_loss: 0.1500 - val_accuracy: 0.9569\n",
            "Epoch 9/10\n",
            "938/938 [==============================] - 19s 20ms/step - loss: 0.1662 - accuracy: 0.9501 - val_loss: 0.3098 - val_accuracy: 0.9063\n",
            "Epoch 10/10\n",
            "938/938 [==============================] - 19s 20ms/step - loss: 0.1521 - accuracy: 0.9538 - val_loss: 0.1620 - val_accuracy: 0.9508\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc3dd191cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.4 Test the Keras Gated Recurrence Unit Model\n",
        "for i in range(10):\n",
        "    result = tf.argmax(model.predict(tf.expand_dims(x_test[i], 0)), axis=1)    \n",
        "    print(result.numpy(), y_test[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lE1bGsNCcPap",
        "outputId": "89e578ff-940a-4e6c-b800-07ddcb7915e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 427ms/step\n",
            "[7] 7\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "[8] 8\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "[9] 9\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "[0] 0\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "[1] 1\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "[2] 2\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "[3] 3\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "[4] 4\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "[5] 5\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "[6] 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation - After 10 epochs, the model does quite well at roughly 95% accuracy for both the training and validation data. It predicts all 10 test data points correctly."
      ],
      "metadata": {
        "id": "ON-cuYlzcYaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM (Long Short Term Memory)\n",
        "<img src=\"https://miro.medium.com/max/1156/1*laH0_xXEkFE0lKJu54gkFQ.png\"><br>\n",
        "The term \"long short-term memory\" comes from the following intuition.\n",
        "Simple recurrent neural networks \n",
        "have *long-term memory* in the form of weights.\n",
        "The weights change slowly during training, \n",
        "encoding general knowledge about the data.\n",
        "They also have *short-term memory*\n",
        "in the form of ephemeral activations,\n",
        "which pass from each node to successive nodes.\n",
        "The LSTM model introduces an intermediate type of storage via the memory cell.\n",
        "A memory cell is a composite unit, \n",
        "built from simpler nodes \n",
        "in a specific connectivity pattern,\n",
        "with the novel inclusion of multiplicative nodes."
      ],
      "metadata": {
        "id": "1l0F5fgpcoQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Vanilla LSTM**"
      ],
      "metadata": {
        "id": "noqAmEKIL9Y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras import losses"
      ],
      "metadata": {
        "id": "VIsGJoGkeFn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# univariate data preparation\n",
        "from numpy import array\n",
        " \n",
        "# split a univariate sequence into samples\n",
        "def split_sequence(sequence, n_steps):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(len(sequence)):\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + n_steps\n",
        "\t\t# check if we are beyond the sequence\n",
        "\t\tif end_ix > len(sequence)-1:\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn array(X), array(y)\n",
        " \n",
        "# define input sequence\n",
        "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
        "# choose a number of time steps\n",
        "n_steps = 3\n",
        "# split into samples\n",
        "X, y = split_sequence(raw_seq, n_steps)\n",
        "# summarize the data\n",
        "for i in range(len(X)):\n",
        "\tprint(X[i], y[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9KEa3h8LAow",
        "outputId": "cfc9f65b-085f-4764-cf69-0482abf6dbf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10 20 30] 40\n",
            "[20 30 40] 50\n",
            "[30 40 50] 60\n",
            "[40 50 60] 70\n",
            "[50 60 70] 80\n",
            "[60 70 80] 90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model with even series performs well with sgd as optimizer and Huber loss. Some part of the code is borrowed rest of the modeling is tried by own adjusting the comination of losses and optimizers"
      ],
      "metadata": {
        "id": "1U8YnFrTS4Wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define input sequence\n",
        "raw_seq = [2,4,6,8,10]\n",
        "# choose a number of time steps\n",
        "n_steps = 3\n",
        "# split into samples\n",
        "X, y = split_sequence(raw_seq, n_steps)\n",
        "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
        "n_features = 1\n",
        "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='sgd', loss=tf.keras.losses.Huber())\n",
        "# fit model\n",
        "model.fit(X, y, epochs=200, verbose=0)\n",
        "# demonstrate prediction\n",
        "x_input = array([8, 10, 12])\n",
        "x_input = x_input.reshape((1, n_steps, n_features))\n",
        "yhat = model.predict(x_input, verbose=0)\n",
        "print(yhat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Caqgo4zHP55g",
        "outputId": "47f8062f-76c4-4d70-adda-89264fa39245"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[13.382292]]\n"
          ]
        }
      ]
    }
  ]
}